
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using custom data configuration default-c30907fc16e79295
Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-c30907fc16e79295/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)
100%|████████████████████████████████████████| 3/3 [00:00<00:00,  5.34ba/s]
100%|████████████████████████████████████████| 1/1 [00:00<00:00, 43.77ba/s]
/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2735
  Num Epochs = 5
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 855
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
















































































 20%|███████▏                            | 170/855 [02:40<11:15,  1.01it/s]
 20%|███████▏                            | 171/855 [02:41<11:01,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1





Configuration saved in kogpt-finetune/checkpoint-171/config.json
(85, 216, 51200)
{'eval_loss': 0.3885331451892853, 'eval_accuracy': 0.6805555555555556, 'eval_runtime': 11.5025, 'eval_samples_per_second': 7.39, 'eval_steps_per_second': 7.39, 'epoch': 1.0}
Model weights saved in kogpt-finetune/checkpoint-171/pytorch_model.bin




















































































 40%|██████████████▎                     | 341/855 [05:57<08:29,  1.01it/s]
 40%|██████████████▍                     | 342/855 [05:58<08:18,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|█████████████████████████████████████▌| 84/85 [00:03<00:00, 14.92it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-342/config.json
Model weights saved in kogpt-finetune/checkpoint-342/pytorch_model.bin




















































































 60%|█████████████████████▌              | 512/855 [09:09<05:39,  1.01it/s]
 60%|█████████████████████▌              | 513/855 [09:10<05:32,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|█████████████████████████████████████▌| 84/85 [00:03<00:00, 16.37it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-513/config.json
Model weights saved in kogpt-finetune/checkpoint-513/pytorch_model.bin










 63%|██████████████████████▌             | 535/855 [09:55<05:29,  1.03s/it]Traceback (most recent call last):
  File "2test.py", line 87, in <module>
    trainer.train()
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 1365, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 1958, in training_step
    loss.backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 147, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt