
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using custom data configuration default-c30907fc16e79295
Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-c30907fc16e79295/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)
100%|████████████████████████████████████████| 3/3 [00:00<00:00,  4.87ba/s]
100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.82ba/s]
/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2735
  Num Epochs = 10
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 1710
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"

















































































 10%|███▍                               | 170/1710 [02:42<25:27,  1.01it/s]
 10%|███▌                               | 171/1710 [02:42<24:58,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1




100%|██████████████████████████████████████| 85/85 [00:07<00:00,  6.19it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-171/config.json
Model weights saved in kogpt-finetune/checkpoint-171/pytorch_model.bin
















































































