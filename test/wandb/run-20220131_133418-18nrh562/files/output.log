
Downloading: 100%|██████████████████████████████████| 0.98k/0.98k [00:00<00:00, 450kB/s]

Downloading: 100%|█████████████████████████████████| 2.69M/2.69M [00:01<00:00, 2.18MB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.



Downloading: 100%|███████████████████████████████████| 490M/490M [00:06<00:00, 79.4MB/s]
Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-c30907fc16e79295/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...
Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c30907fc16e79295/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.
Using custom data configuration default-c30907fc16e79295
100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 5329.48it/s]
100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00, 806.44it/s]
100%|█████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.95ba/s]
100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00, 41.94ba/s]
/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2735
  Num Epochs = 10
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 1710
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"















































































 10%|████▋                                           | 168/1710 [02:38<25:08,  1.02it/s]

 10%|████▊                                           | 171/1710 [02:41<24:41,  1.04it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1



100%|███████████████████████████████████████████████████| 85/85 [00:07<00:00,  6.30it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-171/config.json
Model weights saved in kogpt-finetune/checkpoint-171/pytorch_model.bin




















































































 20%|█████████▌                                      | 341/1710 [05:44<22:52,  1.00s/it]
 20%|█████████▌                                      | 342/1710 [05:45<22:26,  1.02it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


100%|███████████████████████████████████████████████████| 85/85 [00:03<00:00, 14.62it/s]
(85, 216, 51200)
Configuration saved in kogpt-finetune/checkpoint-342/config.json
Model weights saved in kogpt-finetune/checkpoint-342/pytorch_model.bin



















































































 30%|██████████████▍                                 | 513/1710 [08:47<19:23,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
  9%|████▉                                               | 8/85 [00:00<00:02, 34.12it/s]


100%|███████████████████████████████████████████████████| 85/85 [00:03<00:00, 15.80it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-513/config.json
Model weights saved in kogpt-finetune/checkpoint-513/pytorch_model.bin



















































































 40%|███████████████████▏                            | 682/1710 [11:47<16:57,  1.01it/s]
 40%|███████████████████▏                            | 684/1710 [11:49<16:36,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|██████████████████████████████████████████████████▍| 84/85 [00:03<00:00, 15.98it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-684/config.json
Model weights saved in kogpt-finetune/checkpoint-684/pytorch_model.bin




















































































 50%|███████████████████████▉                        | 854/1710 [14:50<14:08,  1.01it/s]
 50%|████████████████████████                        | 855/1710 [14:51<13:51,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|██████████████████████████████████████████████████▍| 84/85 [00:03<00:00, 16.31it/s]
(85, 216, 51200)
Configuration saved in kogpt-finetune/checkpoint-855/config.json
Model weights saved in kogpt-finetune/checkpoint-855/pytorch_model.bin



















































































 60%|████████████████████████████▏                  | 1025/1710 [17:50<11:20,  1.01it/s]
 60%|████████████████████████████▏                  | 1026/1710 [17:51<11:04,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|██████████████████████████████████████████████████▍| 84/85 [00:03<00:00, 16.40it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-1026/config.json
Model weights saved in kogpt-finetune/checkpoint-1026/pytorch_model.bin



















































































 70%|████████████████████████████████▊              | 1195/1710 [20:56<08:30,  1.01it/s]
 70%|████████████████████████████████▉              | 1197/1710 [20:58<08:18,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|██████████████████████████████████████████████████▍| 84/85 [00:03<00:00, 16.39it/s]
(85, 216, 51200)
Configuration saved in kogpt-finetune/checkpoint-1197/config.json
Model weights saved in kogpt-finetune/checkpoint-1197/pytorch_model.bin



















































































 80%|█████████████████████████████████████▌         | 1366/1710 [23:57<05:41,  1.01it/s]
 80%|█████████████████████████████████████▌         | 1368/1710 [23:58<05:33,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|██████████████████████████████████████████████████▍| 84/85 [00:03<00:00, 16.45it/s]
(85, 216, 51200)
Configuration saved in kogpt-finetune/checkpoint-1368/config.json
Model weights saved in kogpt-finetune/checkpoint-1368/pytorch_model.bin



















































































 90%|██████████████████████████████████████████▎    | 1539/1710 [26:59<02:46,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 18%|█████████                                          | 15/85 [00:00<00:01, 38.83it/s]


 99%|██████████████████████████████████████████████████▍| 84/85 [00:03<00:00, 16.44it/s]
(85, 216, 51200)
Configuration saved in kogpt-finetune/checkpoint-1539/config.json
Model weights saved in kogpt-finetune/checkpoint-1539/pytorch_model.bin



















































































100%|██████████████████████████████████████████████▉| 1708/1710 [29:57<00:01,  1.01it/s]
100%|███████████████████████████████████████████████| 1710/1710 [29:59<00:00,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|██████████████████████████████████████████████████▍| 84/85 [00:03<00:00, 16.41it/s]
(85, 216, 51200)
Configuration saved in kogpt-finetune/checkpoint-1710/config.json
Model weights saved in kogpt-finetune/checkpoint-1710/pytorch_model.bin
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from kogpt-finetune/checkpoint-1026 (score: 0.2784672677516937).
{'train_runtime': 1819.0082, 'train_samples_per_second': 15.036, 'train_steps_per_second': 0.94, 'train_loss': 0.23773959979676365, 'epoch': 10.0}
100%|███████████████████████████████████████████████| 1710/1710 [30:18<00:00,  1.06s/it]
Saving model checkpoint to test-kogpt-trained-hchang
Configuration saved in test-kogpt-trained-hchang/config.json
Model weights saved in test-kogpt-trained-hchang/pytorch_model.bin
loading configuration file test-kogpt-trained-hchang/config.json
Model config GPT2Config {
  "_name_or_path": "skt/kogpt2-base-v2",
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "author": "Heewon Jeon(madjakarta@gmail.com)",
  "bos_token_id": 0,
  "created_date": "2021-04-28",
  "embd_pdrop": 0.1,
  "eos_token_id": 1,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "license": "CC-BY-NC-SA 4.0",
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 3,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.16.1",
  "use_cache": true,
  "vocab_size": 51200
}
loading weights file test-kogpt-trained-hchang/pytorch_model.bin
All model checkpoint weights were used when initializing GPT2LMHeadModel.
All the weights of GPT2LMHeadModel were initialized from the model checkpoint at test-kogpt-trained-hchang.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
4 x A = 608 일 때, A에 알맞은 수를 구하세요.
=====
a = 5
b = 608
y = b // a
print(y)
실행결과:
121
두 수 A, B의 합은 63이고 차는 3입니다. 두 수 A, B의 곱을 구하시오.
=====
a = 63
b = 3
c = 3
y = ((a + b) // 2) * c
print(y)
실행결과:
99
어떤 직육면체의 가로의 길이가 6cm, 세로의 길이가 4cm, 높이가 8cm입니다. 이 직육면체의 전체 둘레는 얼마일까요?
=====
a = 6
b = 4
c = 8
d = 8
y = (c + d) * 4
print(y)
실행결과:
64
네 변의 길이의 합이 88cm인 정사각형의 한 변의 길이는 몇 cm인가요?
=====
a = 88
b = 4
y = a // b
print(y)
실행결과:
22
들이가 4L인 물병을 상우는 약 3L 200mL, 서형이는 약 3L 750mL로 어림하였습니다. 실제 들이에 더 가깝게 어림한 사람은 누구일까요?
=====
a = 4 * 1000 + 3 * 1000 + 7000
b = 3 * 1000 + 7000
y = a + b + c
print(y)
실행결과:
error