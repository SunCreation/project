
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using custom data configuration default-c30907fc16e79295
Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-c30907fc16e79295/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)
100%|████████████████████████████████████████| 3/3 [00:00<00:00,  4.84ba/s]
100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.95ba/s]
/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2735
  Num Epochs = 5
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 855
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"


















































































 20%|███████                             | 169/855 [02:44<11:25,  1.00it/s]
 20%|███████▏                            | 171/855 [02:46<11:09,  1.02it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1




100%|██████████████████████████████████████| 85/85 [00:07<00:00,  6.32it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-171/config.json
Model weights saved in kogpt-finetune/checkpoint-171/pytorch_model.bin



















































































 40%|██████████████▎                     | 340/855 [06:00<08:29,  1.01it/s]
 40%|██████████████▍                     | 342/855 [06:02<08:18,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|█████████████████████████████████████▌| 84/85 [00:03<00:00, 14.86it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-342/config.json
Model weights saved in kogpt-finetune/checkpoint-342/pytorch_model.bin



















































































 60%|█████████████████████▌              | 511/855 [09:13<05:40,  1.01it/s]
 60%|█████████████████████▌              | 513/855 [09:15<05:31,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


100%|██████████████████████████████████████| 85/85 [00:03<00:00, 15.74it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-513/config.json
Model weights saved in kogpt-finetune/checkpoint-513/pytorch_model.bin



















































































 80%|████████████████████████████▋       | 682/855 [12:25<02:51,  1.01it/s]
 80%|████████████████████████████▊       | 684/855 [12:27<02:46,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|█████████████████████████████████████▌| 84/85 [00:03<00:00, 16.14it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-684/config.json
Model weights saved in kogpt-finetune/checkpoint-684/pytorch_model.bin




















































































100%|███████████████████████████████████▉| 854/855 [15:28<00:00,  1.01it/s]
100%|████████████████████████████████████| 855/855 [15:29<00:00,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


100%|██████████████████████████████████████| 85/85 [00:03<00:00, 16.16it/s]
(85, 216, 51200)
Configuration saved in kogpt-finetune/checkpoint-855/config.json
Model weights saved in kogpt-finetune/checkpoint-855/pytorch_model.bin
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from kogpt-finetune/checkpoint-855 (score: 0.2504356801509857).
{'train_runtime': 941.5597, 'train_samples_per_second': 14.524, 'train_steps_per_second': 0.908, 'train_loss': 0.35233363993683753, 'epoch': 5.0}
100%|████████████████████████████████████| 855/855 [15:41<00:00,  1.10s/it]
Saving model checkpoint to test-kogpt-trained-hchang
Configuration saved in test-kogpt-trained-hchang/config.json
Model weights saved in test-kogpt-trained-hchang/pytorch_model.bin
loading configuration file test-kogpt-trained-hchang/config.json
Model config GPT2Config {
  "_name_or_path": "skt/kogpt2-base-v2",
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "author": "Heewon Jeon(madjakarta@gmail.com)",
  "bos_token_id": 0,
  "created_date": "2021-04-28",
  "embd_pdrop": 0.1,
  "eos_token_id": 1,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "license": "CC-BY-NC-SA 4.0",
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 3,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.16.1",
  "use_cache": true,
  "vocab_size": 51200
}
loading weights file test-kogpt-trained-hchang/pytorch_model.bin
All model checkpoint weights were used when initializing GPT2LMHeadModel.
All the weights of GPT2LMHeadModel were initialized from the model checkpoint at test-kogpt-trained-hchang.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
4 x A = 608 일 때, A에 알맞은 수를 구하세요.
=====
a = 4
b = 608
y = b // a
print(y)
실행결과:
152
두 수 A, B의 합은 63이고 차는 3입니다. 두 수 A, B의 곱을 구하시오.
=====
a = 63
b = 3
y = a // b + 1
print(y)
실행결과:
22
어떤 직육면체의 가로의 길이가 6cm, 세로의 길이가 4cm, 높이가 8cm입니다. 이 직육면체의 전체 둘레는 얼마일까요?
=====
a = 6
b = 4
c = 4
d = 8
y = (a + b) * 2 // c
print(y)
실행결과:
5
네 변의 길이의 합이 88cm인 정사각형의 한 변의 길이는 몇 cm인가요?
=====
a = 88
b = 4
y = a // b
print(y)
실행결과:
22
들이가 4L인 물병을 상우는 약 3L 200mL, 서형이는 약 3L 750mL로 어림하였습니다. 실제 들이에 더 가깝게 어림한 사람은 누구일까요?
=====
a = 4
b = 3
c = 7
dicts = {'물병': a * b, '서형': c * d}
y = max(dicts, key=dicts.get)
print(y)
실행결과:
error