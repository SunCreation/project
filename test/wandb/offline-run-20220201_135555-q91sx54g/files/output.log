유주는 1000원짜리 지폐 10장, 500원짜리 동전 5개를 가지고 있습니다. 매일 2500원씩 쓴다면 모두 며칠 동안 쓸 수 있습니까? a = 1000
b = 10
c = 500
d = 5
e = 2500
y = ((a * b) + (c * d)) // e
print(y)<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
{'loss': 0.7129, 'learning_rate': 4.5e-05, 'epoch': 1.0}
(85, 216, 51200)
{'eval_loss': 0.4051354229450226, 'eval_accuracy': 0.6759259259259259, 'eval_runtime': 11.1501, 'eval_samples_per_second': 7.623, 'eval_steps_per_second': 7.623, 'epoch': 1.0}
{'loss': 0.3413, 'learning_rate': 4e-05, 'epoch': 2.0}
(85, 216, 51200)
{'eval_loss': 0.35567766427993774, 'eval_accuracy': 0.6759259259259259, 'eval_runtime': 7.4063, 'eval_samples_per_second': 11.477, 'eval_steps_per_second': 11.477, 'epoch': 2.0}
{'loss': 0.2719, 'learning_rate': 3.5e-05, 'epoch': 3.0}
(85, 216, 51200)
{'eval_loss': 0.3357890844345093, 'eval_accuracy': 0.6759259259259259, 'eval_runtime': 7.2512, 'eval_samples_per_second': 11.722, 'eval_steps_per_second': 11.722, 'epoch': 3.0}
{'loss': 0.2235, 'learning_rate': 3e-05, 'epoch': 4.0}
(85, 216, 51200)
{'eval_loss': 0.3300570249557495, 'eval_accuracy': 0.6759259259259259, 'eval_runtime': 8.0284, 'eval_samples_per_second': 10.587, 'eval_steps_per_second': 10.587, 'epoch': 4.0}
{'loss': 0.1879, 'learning_rate': 2.5e-05, 'epoch': 5.0}
(85, 216, 51200)
{'eval_loss': 0.32456421852111816, 'eval_accuracy': 0.6759259259259259, 'eval_runtime': 7.3269, 'eval_samples_per_second': 11.601, 'eval_steps_per_second': 11.601, 'epoch': 5.0}
{'loss': 0.1607, 'learning_rate': 2e-05, 'epoch': 6.0}
(85, 216, 51200)
{'eval_loss': 0.32606709003448486, 'eval_accuracy': 0.6759259259259259, 'eval_runtime': 7.2976, 'eval_samples_per_second': 11.648, 'eval_steps_per_second': 11.648, 'epoch': 6.0}
{'loss': 0.1401, 'learning_rate': 1.5e-05, 'epoch': 7.0}
(85, 216, 51200)
{'eval_loss': 0.32034415006637573, 'eval_accuracy': 0.6759259259259259, 'eval_runtime': 7.2199, 'eval_samples_per_second': 11.773, 'eval_steps_per_second': 11.773, 'epoch': 7.0}
{'loss': 0.123, 'learning_rate': 1e-05, 'epoch': 8.0}
(85, 216, 51200)
{'eval_loss': 0.32040485739707947, 'eval_accuracy': 0.6759259259259259, 'eval_runtime': 7.196, 'eval_samples_per_second': 11.812, 'eval_steps_per_second': 11.812, 'epoch': 8.0}
{'loss': 0.1109, 'learning_rate': 5e-06, 'epoch': 9.0}
(85, 216, 51200)
{'eval_loss': 0.3213352859020233, 'eval_accuracy': 0.6759259259259259, 'eval_runtime': 7.1698, 'eval_samples_per_second': 11.855, 'eval_steps_per_second': 11.855, 'epoch': 9.0}
{'loss': 0.1029, 'learning_rate': 0.0, 'epoch': 10.0}
(85, 216, 51200)
{'eval_loss': 0.32038143277168274, 'eval_accuracy': 0.6759259259259259, 'eval_runtime': 7.2506, 'eval_samples_per_second': 11.723, 'eval_steps_per_second': 11.723, 'epoch': 10.0}
{'train_runtime': 1927.1415, 'train_samples_per_second': 14.192, 'train_steps_per_second': 0.887, 'train_loss': 0.23752074102212115, 'epoch': 10.0}
4 x A = 608 일 때, A에 알맞은 수를 구하세요.<sys>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using custom data configuration default-c30907fc16e79295
Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-c30907fc16e79295/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)
100%|████████████████████| 3/3 [00:00<00:00,  4.68ba/s]
100%|████████████████████| 1/1 [00:00<00:00, 41.99ba/s]
/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2735
  Num Epochs = 10
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 1710
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
 10%|█▌             | 171/1710 [02:49<24:53,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 10%|█▌             | 171/1710 [03:00<24:53,  1.03it/s]Saving model checkpoint to kogpt-finetune/checkpoint-171
Configuration saved in kogpt-finetune/checkpoint-171/config.json
Model weights saved in kogpt-finetune/checkpoint-171/pytorch_model.bin
 20%|███            | 342/1710 [06:05<22:07,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 20%|███            | 342/1710 [06:12<22:07,  1.03it/s]Saving model checkpoint to kogpt-finetune/checkpoint-342
Configuration saved in kogpt-finetune/checkpoint-342/config.json
Model weights saved in kogpt-finetune/checkpoint-342/pytorch_model.bin
 30%|████▌          | 513/1710 [09:17<19:26,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 30%|████▌          | 513/1710 [09:25<19:26,  1.03it/s]               Saving model checkpoint to kogpt-finetune/checkpoint-513
Configuration saved in kogpt-finetune/checkpoint-513/config.json
Model weights saved in kogpt-finetune/checkpoint-513/pytorch_model.bin
 40%|██████         | 684/1710 [12:29<16:39,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 40%|██████         | 684/1710 [12:37<16:39,  1.03it/Saving model checkpoint to kogpt-finetune/checkpoint-684
Configuration saved in kogpt-finetune/checkpoint-684/config.json
Model weights saved in kogpt-finetune/checkpoint-684/pytorch_model.bin
 50%|███████▌       | 855/1710 [15:43<13:52,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 50%|███████▌       | 855/1710 [15:51<13:52,  1.03it/Saving model checkpoint to kogpt-finetune/checkpoint-855
Configuration saved in kogpt-finetune/checkpoint-855/config.json
Model weights saved in kogpt-finetune/checkpoint-855/pytorch_model.bin
 60%|████████▍     | 1026/1710 [18:55<11:01,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 60%|████████▍     | 1026/1710 [19:03<11:01,  1.03it/Saving model checkpoint to kogpt-finetune/checkpoint-1026
Configuration saved in kogpt-finetune/checkpoint-1026/config.json
Model weights saved in kogpt-finetune/checkpoint-1026/pytorch_model.bin
 70%|█████████▊    | 1197/1710 [22:07<08:19,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 70%|█████████▊    | 1197/1710 [22:14<08:19,  1.03it/Saving model checkpoint to kogpt-finetune/checkpoint-1197
Configuration saved in kogpt-finetune/checkpoint-1197/config.json
Model weights saved in kogpt-finetune/checkpoint-1197/pytorch_model.bin
 80%|███████████▏  | 1368/1710 [25:18<05:32,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 80%|███████████▏  | 1368/1710 [25:25<05:32,  1.03it/Saving model checkpoint to kogpt-finetune/checkpoint-1368
Configuration saved in kogpt-finetune/checkpoint-1368/config.json
Model weights saved in kogpt-finetune/checkpoint-1368/pytorch_model.bin
 90%|████████████▌ | 1539/1710 [28:29<02:45,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 90%|████████████▌ | 1539/1710 [28:37<02:45,  1.03it/Saving model checkpoint to kogpt-finetune/checkpoint-1539
Configuration saved in kogpt-finetune/checkpoint-1539/config.json
Model weights saved in kogpt-finetune/checkpoint-1539/pytorch_model.bin
100%|██████████████| 1710/1710 [31:40<00:00,  1.04it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
100%|██████████████| 1710/1710 [31:48<00:00,  1.04it/Saving model checkpoint to kogpt-finetune/checkpoint-1710
Configuration saved in kogpt-finetune/checkpoint-1710/config.json
Model weights saved in kogpt-finetune/checkpoint-1710/pytorch_model.bin
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from kogpt-finetune/checkpoint-1197 (score: 0.32034415006637573).
100%|██████████████| 1710/1710 [32:07<00:00,  1.13s/it]
Traceback (most recent call last):
  File "2test.py", line 132, in <module>
    solve_problem(p)
  File "2test.py", line 108, in solve_problem
    output = model.generate(input_ids, max_length = 216)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py", line 1173, in generate
    return self.greedy_search(
  File "/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py", line 1469, in greedy_search
    outputs = self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1047, in forward
    transformer_outputs = self.transformer(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 833, in forward
    inputs_embeds = self.wte(input_ids)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 158, in forward
    return F.embedding(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 2043, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)