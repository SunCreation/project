
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using custom data configuration default-c30907fc16e79295
Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-c30907fc16e79295/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)
100%|████████████████████████████████████████| 3/3 [00:00<00:00,  5.31ba/s]
100%|████████████████████████████████████████| 1/1 [00:00<00:00, 35.39ba/s]
/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2735
  Num Epochs = 3
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 513
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"













































































 33%|███████████▊                        | 169/513 [02:33<05:36,  1.02it/s]
 33%|████████████                        | 171/513 [02:35<05:29,  1.04it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1





100%|██████████████████████████████████████| 85/85 [00:07<00:00,  6.16it/s]
(85, 216, 51200)
 33%|████████████                        | 171/513 [02:46<05:29,  1.04it/s]Saving model checkpoint to kogpt-finetune/checkpoint-171
Configuration saved in kogpt-finetune/checkpoint-171/config.json
Model weights saved in kogpt-finetune/checkpoint-171/pytorch_model.bin



















































































 66%|███████████████████████▊            | 340/513 [05:38<02:51,  1.01it/s]
 67%|████████████████████████            | 342/513 [05:40<02:45,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|█████████████████████████████████████▌| 84/85 [00:03<00:00, 14.82it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-342/config.json
Model weights saved in kogpt-finetune/checkpoint-342/pytorch_model.bin




















































































100%|███████████████████████████████████▉| 512/513 [08:45<00:00,  1.01it/s]
100%|████████████████████████████████████| 513/513 [08:46<00:00,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


100%|██████████████████████████████████████| 85/85 [00:03<00:00, 16.05it/s]
(85, 216, 51200)
Configuration saved in kogpt-finetune/checkpoint-513/config.json
Model weights saved in kogpt-finetune/checkpoint-513/pytorch_model.bin
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from kogpt-finetune/checkpoint-513 (score: 0.32973456382751465).
100%|████████████████████████████████████| 513/513 [08:59<00:00,  1.05s/it]
Saving model checkpoint to test-kogpt-trained-hchang
Configuration saved in test-kogpt-trained-hchang/config.json
{'train_runtime': 539.4065, 'train_samples_per_second': 15.211, 'train_steps_per_second': 0.951, 'train_loss': 0.44454022085922273, 'epoch': 3.0}
Model weights saved in test-kogpt-trained-hchang/pytorch_model.bin
loading configuration file test-kogpt-trained-hchang/config.json
Model config GPT2Config {
  "_name_or_path": "skt/kogpt2-base-v2",
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "author": "Heewon Jeon(madjakarta@gmail.com)",
  "bos_token_id": 0,
  "created_date": "2021-04-28",
  "embd_pdrop": 0.1,
  "eos_token_id": 1,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "license": "CC-BY-NC-SA 4.0",
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 3,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.16.1",
  "use_cache": true,
  "vocab_size": 51200
}
loading weights file test-kogpt-trained-hchang/pytorch_model.bin
4 x A = 608 일 때, A에 알맞은 수를 구하세요.
All model checkpoint weights were used when initializing GPT2LMHeadModel.
All the weights of GPT2LMHeadModel were initialized from the model checkpoint at test-kogpt-trained-hchang.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
=====
a = 1
b = 608
y = a * b
print(y)
실행결과:
608
두 수 A, B의 합은 63이고 차는 3입니다. 두 수 A, B의 곱을 구하시오.
=====
a = 63
b = 3
y = a // b
print(y)
실행결과:
21
어떤 직육면체의 가로의 길이가 6cm, 세로의 길이가 4cm, 높이가 8cm입니다. 이 직육면체의 전체 둘레는 얼마일까요?
=====
a = 6
b = 4
c = 8
d = 8
y = (a * b) + (c * d)
print(y)
실행결과:
88
네 변의 길이의 합이 88cm인 정사각형의 한 변의 길이는 몇 cm인가요?
=====
a = 88
b = 4
y = a // b
print(y)
실행결과:
22
들이가 4L인 물병을 상우는 약 3L 200mL, 서형이는 약 3L 750mL로 어림하였습니다. 실제 들이에 더 가깝게 어림한 사람은 누구일까요?
=====
a = 4
b = 3
c = 750
dicts = {'물병': a, '서형': b}
y = max(dicts, key=dicts.get)
print(y)
실행결과:
물병