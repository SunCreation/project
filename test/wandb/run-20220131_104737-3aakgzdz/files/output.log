
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using custom data configuration default-c30907fc16e79295
Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-c30907fc16e79295/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)
100%|████████████████████████████████████████| 3/3 [00:00<00:00,  5.27ba/s]
100%|████████████████████████████████████████| 1/1 [00:00<00:00, 45.76ba/s]
/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2735
  Num Epochs = 5
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 855
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"




















































































 20%|███████                             | 169/855 [02:47<11:17,  1.01it/s]
 20%|███████▏                            | 171/855 [02:49<11:03,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1





100%|██████████████████████████████████████| 85/85 [00:07<00:00,  6.35it/s]
(85, 216, 51200)
 20%|███████▏                            | 171/855 [03:01<11:03,  1.03it/s]Saving model checkpoint to kogpt-finetune/checkpoint-171
Configuration saved in kogpt-finetune/checkpoint-171/config.json
Model weights saved in kogpt-finetune/checkpoint-171/pytorch_model.bin




















































































 40%|██████████████▎                     | 341/855 [06:05<08:29,  1.01it/s]
 40%|██████████████▍                     | 342/855 [06:06<08:18,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1



 99%|█████████████████████████████████████▌| 84/85 [00:03<00:00, 14.78it/s]
(85, 216, 51200)
 40%|██████████████▍                     | 342/855 [06:13<08:18,  1.03it/s]Saving model checkpoint to kogpt-finetune/checkpoint-342
Configuration saved in kogpt-finetune/checkpoint-342/config.json
Model weights saved in kogpt-finetune/checkpoint-342/pytorch_model.bin



















































































 60%|█████████████████████▌              | 511/855 [09:16<05:40,  1.01it/s]
 60%|█████████████████████▌              | 513/855 [09:18<05:32,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1


 99%|█████████████████████████████████████▌| 84/85 [00:03<00:00, 16.30it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-513/config.json
Model weights saved in kogpt-finetune/checkpoint-513/pytorch_model.bin



















































































 80%|████████████████████████████▊       | 684/855 [12:31<02:45,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 19%|███████▏                              | 16/85 [00:00<00:01, 37.96it/s]


100%|██████████████████████████████████████| 85/85 [00:03<00:00, 16.08it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-684/config.json
Model weights saved in kogpt-finetune/checkpoint-684/pytorch_model.bin



















































































100%|████████████████████████████████████| 855/855 [15:43<00:00,  1.03it/s]***** Running Evaluation *****
  Num examples = 85
  Batch size = 1
 25%|█████████▍                            | 21/85 [00:00<00:01, 32.18it/s]


 99%|█████████████████████████████████████▌| 84/85 [00:03<00:00, 16.21it/s]
(85, 216, 51200)

Configuration saved in kogpt-finetune/checkpoint-855/config.json
Model weights saved in kogpt-finetune/checkpoint-855/pytorch_model.bin
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from kogpt-finetune/checkpoint-855 (score: 0.3131692409515381).
{'train_runtime': 966.7407, 'train_samples_per_second': 14.145, 'train_steps_per_second': 0.884, 'train_loss': 0.34938875611065423, 'epoch': 5.0}
100%|████████████████████████████████████| 855/855 [16:06<00:00,  1.13s/it]
Saving model checkpoint to test-kogpt-trained-hchang
Configuration saved in test-kogpt-trained-hchang/config.json
Model weights saved in test-kogpt-trained-hchang/pytorch_model.bin
loading configuration file test-kogpt-trained-hchang/config.json
Model config GPT2Config {
  "_name_or_path": "skt/kogpt2-base-v2",
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "author": "Heewon Jeon(madjakarta@gmail.com)",
  "bos_token_id": 0,
  "created_date": "2021-04-28",
  "embd_pdrop": 0.1,
  "eos_token_id": 1,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "license": "CC-BY-NC-SA 4.0",
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 3,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.16.1",
  "use_cache": true,
  "vocab_size": 51200
}
loading weights file test-kogpt-trained-hchang/pytorch_model.bin
All model checkpoint weights were used when initializing GPT2LMHeadModel.
All the weights of GPT2LMHeadModel were initialized from the model checkpoint at test-kogpt-trained-hchang.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
4 x A = 608 일 때, A에 알맞은 수를 구하세요.
=====
a = 4
b = 608
y = a - b
print(y)
실행결과:
-604
두 수 A, B의 합은 63이고 차는 3입니다. 두 수 A, B의 곱을 구하시오.
=====
a = 63
b = 3
c = a // b
d = a + b
y = c + d
print(y)
실행결과:
87
어떤 직육면체의 가로의 길이가 6cm, 세로의 길이가 4cm, 높이가 8cm입니다. 이 직육면체의 전체 둘레는 얼마일까요?
=====
a = 6
b = 4
c = 8
y = (c - a * b) // a
print(y)
실행결과:
-3
네 변의 길이의 합이 88cm인 정사각형의 한 변의 길이는 몇 cm인가요?
=====
a = 88
b = 4
y = a // b
print(y)
실행결과:
22
들이가 4L인 물병을 상우는 약 3L 200mL, 서형이는 약 3L 750mL로 어림하였습니다. 실제 들이에 더 가깝게 어림한 사람은 누구일까요?
=====
a = 4
b = 3
c = 3
d = 750
y = int(d / c)
print(y)
실행결과:
250